# Model configuration
model_name: "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"
peft_config: "lora"
lora_r: 64
lora_alpha: 32
lora_dropout: 0.05
target_modules: ["q_proj", "v_proj"]

# Training configuration
learning_rate: 2e-5
weight_decay: 0.01
warmup_steps: 1000
max_steps: 10000
gradient_accumulation_steps: 32
max_grad_norm: 1.0

# Data configuration
max_length: 2048
batch_size: 1
eval_batch_size: 4

# Optimization configuration
mixed_precision: "no"
gradient_checkpointing: true

# Quantization configuration
load_in_4bit: false
bnb_4bit_compute_dtype: "float32"
bnb_4bit_quant_type: "nf4"

# Directories
output_dir: "outputs"
logging_dir: "logs"

# Miscellaneous
seed: 42
logging_steps: 10
eval_steps: 100
save_steps: 1000
num_workers: 4 