model_name: TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T
peft_config: lora
lora_r: 64
lora_alpha: 32
lora_dropout: 0.05
target_modules:
- q_proj
- v_proj
learning_rate: 2.0e-05
weight_decay: 0.01
warmup_steps: 1000
max_steps: 10000
gradient_accumulation_steps: 32
max_grad_norm: 1.0
max_length: 2048
batch_size: 1
eval_batch_size: 4
mixed_precision: 'no'
gradient_checkpointing: true
load_in_4bit: false
bnb_4bit_compute_dtype: float32
bnb_4bit_quant_type: nf4
output_dir: outputs
logging_dir: logs
seed: 42
logging_steps: 10
eval_steps: 100
save_steps: 1000
num_workers: 4
